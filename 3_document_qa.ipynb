{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "TbEyM5LG7v"
      },
      "source": [
        "# Dependencies\n",
        "\n",
        "1. PyPDFLoader depends upon pypdf to process the pdfs\n",
        "2. YoutubeAudioLoader depends upon yt_dlp, pydub and librosa\n",
        "    - yt_dlp: To download the relevant audio transcripts of youtube videos\n",
        "    - pydub: To split the audio to adhere to OpenAI Whisper's 25mb limit\n",
        "\n",
        "(Here is the relevant list of all other document loaders)[https://python.langchain.com/docs/integrations/document_loaders]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "jROmUVO56V"
      },
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "pdf_loader = PyPDFLoader('docs/pdf/hyperion.pdf')\n",
        "pdf_docs = pdf_loader.load()\n",
        "pdf_docs = pdf_docs\n",
        "print(f'Number of pages: {len(pdf_docs)}')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Number of pages: 570\n"
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "S1hZu5RQBT"
      },
      "source": [
        "# Possible Bugs\n",
        "\n",
        "I've encountered the recent versions of openai (>=1.0.0) to be incompatible with the latest version of langchain (0.0.333), as a result I've had to make the following changes:\n",
        "\n",
        "1. run `openai migrate <path-to-langchain>/document_loaders/parsers/audio.py` replace `<path-to-langchain>` with the correct path to langchain\n",
        "2. change line 66 in `<path-to-langchain>/document_loaders/parsers/audio.py` to `transcript = client.audio.transcriptions.create(model=\"whisper-1\", file=file_obj)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "XoRGWAQZ64"
      },
      "source": [
        "from langchain.document_loaders.blob_loaders.youtube_audio import YoutubeAudioLoader\n",
        "from langchain.document_loaders.generic import GenericLoader\n",
        "from langchain.document_loaders.parsers import OpenAIWhisperParser\n",
        "\n",
        "url = 'https://youtu.be/_PPWWRV6gbA?si=hQFeGBgt6yawfuPI'\n",
        "youtube_loader = GenericLoader(YoutubeAudioLoader([url],'docs/youtube'), OpenAIWhisperParser())\n",
        "youtube_docs = youtube_loader.load()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "Ms4oGwnKyA"
      },
      "source": [
        "# Preprocessing & Splitting\n",
        "\n",
        "### Preprocessing\n",
        "1. The `PyPDFLoader` returns a list of `Document` objects, each of which has a `page_content` and `metadata` attribute\n",
        "2. The `page_content` attribute is then preprocessed to add `#` before each chapter number\n",
        "3. The `metadata` attribute is then updated to include the chapter number, story and character name\n",
        "\n",
        "\n",
        "### Splitting\n",
        "Langchain provides us with numerous splitting options, some of most common ones are:\n",
        "1. `CharacterTextSplitter`: Splits the text into chunks of a fixed size, with a fixed overlap\n",
        "2. `RecursiveCharacterTextSplitter`: Simillar to `CharacterTextSplitter` but recursively splits the text into smaller chunks\n",
        "3. `MarkdownTextSplitter`: Splits the text into chunks based on markdown headers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "Cc7oqpJZQe"
      },
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import re\n",
        "\n",
        "def preprocess_pdf(docs):\n",
        "    sc_mapping = {'Priest/Lenar Hoyt': [35, 114], 'Soldier/Fedmahn Kassad': [144, 204], 'Poet/Martin Silenus': [210, 271], 'Scholar/Sol Weintraub': [285, 356], 'Detective/Brawne Lamia': [376, 470], 'Consul/Consul': [484, 541]}\n",
        "    chapter = 0\n",
        "    for page in docs:\n",
        "        page_content = re.sub(r'^\\s*(\\d+)\\s*$', r'#\\1', page.page_content, flags=re.MULTILINE)\n",
        "        if '#' in page_content: chapter = int(page_content.split('#')[1].split('\\n')[0])\n",
        "        for k,v in sc_mapping.items():\n",
        "            if v[0] <= page.metadata['page']+1 <= v[1]:\n",
        "                story = k.split('/')[0]\n",
        "                character = k.split('/')[1]\n",
        "                break\n",
        "            story, character = 'Plot', 'None'\n",
        "        page.page_content = page_content\n",
        "        page.metadata['chapter'] = chapter\n",
        "        page.metadata['story'] = story\n",
        "        page.metadata['character'] = character\n",
        "        page.metadata['page'] += 1\n",
        "    return docs\n",
        "\n",
        "pdf_docs = preprocess_pdf(pdf_docs)\n",
        "pdf_docs_copy = pdf_docs.copy()\n",
        "for _ in range(3):\n",
        "    pdf_docs += pdf_docs_copy.copy()\n",
        "print(f'Number of pages: {len(pdf_docs)}')\n",
        "print(f'random page: {pdf_docs[320].page_content[:100]}')\n",
        "print(f'random page metadata: {pdf_docs[320].metadata}')\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1500,\n",
        "    chunk_overlap=150,\n",
        "    separators=['#', '\\n', '\"', ' ', '']\n",
        ")\n",
        "splits = text_splitter.split_documents(pdf_docs)\n",
        "print(f'Number of splits: {len(splits)}')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Number of pages: 2280\nrandom page: last contact, Arundez had aged but little\u2014Sol guessed that\nhe was still in his late twenties. But th\nrandom page metadata: {'source': 'docs/pdf/hyperion.pdf', 'page': 321, 'chapter': 4, 'story': 'Scholar', 'character': 'Sol Weintraub'}\nNumber of splits: 4340\n"
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "WedNBTtFMw"
      },
      "source": [
        "# Embedding & Vectorstore\n",
        "\n",
        "### Embedding\n",
        "Langchain provides us with numerous vectorization options, some of most common ones are:\n",
        "1. `HuggingFaceEmbeddings`: Uses the HuggingFace transformers library to generate embeddings\n",
        "2. `OpenAIEmbeddings`: Uses the OpenAI GPT library to generate embeddings\n",
        "\n",
        "We chose to use the `HuggingFaceEmbeddings` as OpenAI was rate limiting the number of requests we could make to their API\n",
        "\n",
        "### Vectorstore\n",
        "A vectorstore is a database of embeddings which corresponds to a set of documents. Langchain provides us with numerous vectorstore options, some of most common ones are:\n",
        "1. `FAISS`: Stands for Facebook AI similarity search\n",
        "2. `Chroma`: langchain's preferred vectorstore"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "olTlw6cSCu"
      },
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS, Chroma\n",
        "import os\n",
        "\n",
        "embedding = HuggingFaceEmbeddings()\n",
        "save_dir = r'docs/vectorstores'\n",
        "\n",
        "def retrieve_FAISS_vectorstore(documents):\n",
        "    store_name, _ = os.path.splitext(os.path.basename(documents[0].metadata['source']))\n",
        "    store_path = os.path.join(os.getcwd(), save_dir, 'faiss', store_name)\n",
        "    if not os.path.exists(store_path):\n",
        "        vectorstore = FAISS.from_documents(documents=documents, embedding=embedding)\n",
        "        vectorstore.save_local(store_path)\n",
        "        return vectorstore\n",
        "    else:\n",
        "        return FAISS.load_local(store_path, embedding)\n",
        "\n",
        "def retrieve_Chroma_vectorstore(documents):\n",
        "    store_name, _ = os.path.splitext(os.path.basename(documents[0].metadata['source']))\n",
        "    store_path = os.path.join(os.getcwd(), save_dir, 'chroma', store_name)\n",
        "    if not os.path.exists(store_path):\n",
        "        os.makedirs(store_path)\n",
        "        vectorstore = Chroma.from_documents(documents=documents, embedding=embedding, persist_directory=store_path)\n",
        "    else:\n",
        "        vectorstore = Chroma(persist_directory=store_path, embedding_function=embedding)\n",
        "    return vectorstore\n",
        "\n",
        "\n",
        "vectordb = retrieve_Chroma_vectorstore(splits)\n",
        "query = 'Who are the Ousters?'\n",
        "docs_ss = vectordb.similarity_search(query, k=2)\n",
        "print(docs_ss)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[Document(page_content='the Ousters. The SDF forces have been running wild. Much of\\nthe carnage could be their doing.\u201d\\n\u201cWith no bodies?\u201d laughed Martin Silenus. \u201cWishful\\nthinking. Our absent hosts downstairs dangle now on the\\nShrike\u2019s steel tree. Where, ere long, we too will be.\u201d\\n\u201cShut up,\u201d Brawne Lamia said tiredly.', metadata={'chapter': 6, 'character': 'None', 'page': 476, 'source': 'docs/pdf/hyperion.pdf', 'story': 'Plot'}), Document(page_content='the Ousters. The SDF forces have been running wild. Much of\\nthe carnage could be their doing.\u201d\\n\u201cWith no bodies?\u201d laughed Martin Silenus. \u201cWishful\\nthinking. Our absent hosts downstairs dangle now on the\\nShrike\u2019s steel tree. Where, ere long, we too will be.\u201d\\n\u201cShut up,\u201d Brawne Lamia said tiredly.', metadata={'chapter': 6, 'character': 'None', 'page': 476, 'source': 'docs/pdf/hyperion.pdf', 'story': 'Plot'})]\n"
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "g5vkwfMIU1"
      },
      "source": [
        "# Max Marginal Relevance\n",
        "To introduce diversity in the model responses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "wNICNzcrFr"
      },
      "source": [
        "docs_mmr = vectordb.max_marginal_relevance_search(query, k=2, fetch_k=5)\n",
        "for i in range(2):\n",
        "    print(f'{i+1}. Similarity search: {docs_ss[i].metadata}')\n",
        "    print(f'{i+1}. Marginal relevance: {docs_mmr[i].metadata}')\n",
        "    print('')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "1. Similarity search: {'chapter': 0, 'character': 'None', 'page': 12, 'source': 'docs/pdf/hyperion.pdf', 'story': 'Plot'}\n1. Marginal relevance: {'chapter': 0, 'character': 'None', 'page': 12, 'source': 'docs/pdf/hyperion.pdf', 'story': 'Plot'}\n\n2. Similarity search: {'chapter': 0, 'character': 'None', 'page': 12, 'source': 'docs/pdf/hyperion.pdf', 'story': 'Plot'}\n2. Marginal relevance: {'chapter': 0, 'character': 'None', 'page': 13, 'source': 'docs/pdf/hyperion.pdf', 'story': 'Plot'}\n\n"
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "1xz9Ar6H14"
      },
      "source": [
        "**Note**: Unfortunately, there was no way around using openai ver. 0.28 for the following part. Please downgrade it with the command pip install openai==0.28\n",
        "\n",
        "### Self Query\n",
        "Self query allows langchain to auto infer the metadata filter as well as the query which results in better inference overall\n",
        "\n",
        "For example: The query 'What are some movies about aliens made in 1980' would be passed through an llm and divided into te following parts:\n",
        "1. `query`: alien movies\n",
        "2. `metadata filter`: eq(\"year\" 1980)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "Rr4bA8pa9t"
      },
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
        "from langchain.chains.query_constructor.base import AttributeInfo\n",
        "\n",
        "metadata_field_info = [\n",
        "    AttributeInfo(\n",
        "        name = 'source',\n",
        "        description = 'File path of the document',\n",
        "        type = 'text',\n",
        "    ),\n",
        "    AttributeInfo(\n",
        "        name = 'chapter',\n",
        "        description = 'Chapter number of the 579-page document ranging from 0-6',\n",
        "        type = 'integer',\n",
        "    ),\n",
        "    AttributeInfo(\n",
        "        name = 'page',\n",
        "        description = 'Page number of the document ranging from 0-579',\n",
        "        type = 'integer',\n",
        "    ),\n",
        "    AttributeInfo(\n",
        "        name = 'story',\n",
        "        description = 'Profession of the character who is narrating their story. Plot is used in the case no character is narrating their story. Possible values are: Priest, Soldier, Poet, Scholar, Detective, Consul, Plot',\n",
        "        type = 'text',\n",
        "    ),\n",
        "    AttributeInfo(\n",
        "        name = 'character',\n",
        "        description = 'Name of the character who is narrating their story. None is used in the case no character is narrating their story. Possible values are: Lenar Hoyt, Fedmahn Kassad, Martin Silenus, Sol Weintraub, Brawne Lamia, Consul, None',\n",
        "        type = 'text',\n",
        "    ),\n",
        "]\n",
        "document_content_description = 'Hyperion book by Dan Simmons'\n",
        "llm = OpenAI(temperature=0)\n",
        "retriever = SelfQueryRetriever.from_llm(\n",
        "    llm,\n",
        "    vectordb,\n",
        "    document_content_description,\n",
        "    metadata_field_info,\n",
        "    verbose=True,\n",
        ")\n",
        "query_2 = 'Who did Lenar Hoyt find while narrating his story?'\n",
        "docs_ss_2 = vectordb.similarity_search(query_2, k=2)\n",
        "docs_sqr = retriever.get_relevant_documents(query_2)\n",
        "print('Similarity search:\\n', [doc.metadata for doc in docs_ss_2])\n",
        "print('Self query retrieval:\\n', [doc.metadata for doc in docs_sqr])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Similarity search:\n [{'chapter': 5, 'character': 'None', 'page': 361, 'source': 'docs/pdf/hyperion.pdf', 'story': 'Plot'}, {'chapter': 5, 'character': 'None', 'page': 361, 'source': 'docs/pdf/hyperion.pdf', 'story': 'Plot'}]\nSelf query retrieval:\n [{'chapter': 1, 'character': 'Lenar Hoyt', 'page': 37, 'source': 'docs/pdf/hyperion.pdf', 'story': 'Priest'}, {'chapter': 1, 'character': 'Lenar Hoyt', 'page': 37, 'source': 'docs/pdf/hyperion.pdf', 'story': 'Priest'}, {'chapter': 1, 'character': 'Lenar Hoyt', 'page': 37, 'source': 'docs/pdf/hyperion.pdf', 'story': 'Priest'}, {'chapter': 1, 'character': 'Lenar Hoyt', 'page': 37, 'source': 'docs/pdf/hyperion.pdf', 'story': 'Priest'}]\n"
        }
      ],
      "execution_count": 5
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "python",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}